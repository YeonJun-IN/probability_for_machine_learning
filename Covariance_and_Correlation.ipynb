{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covariance and Correlation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA9onLwgUjKhZTRjB3WFGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeonJun-IN/probability_for_machine_learning/blob/main/Covariance_and_Correlation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfNXGCqRG7dr"
      },
      "source": [
        "# Covariance\n",
        "## 1. Definition\n",
        "- 두 확률 변수 $X, Y$가 각각의 평균으로부터 얼마나 퍼져있는지를 보여주는 척도. \n",
        "\n",
        "- X의 값이 커지거나 작아질 때 Y의 값도 그에 따라서 커지거나 작아지는 경향을 보이면 양의 값을 가지고, X의 값이 커지거나 작아질 때 Y의 값이 그와 반대로 작아지거나 커지는 경향을 보이면 음의 값을 가지게 된다. \n",
        "\n",
        "- 따라서 Covariance 값의 부호(sign)은 두 확률변수의 선형 관계의 경향성을 나타낸다. \n",
        "- 확률 변수의 magnitude에 Covariance 값이 영향을 많이 받으므로, Covariance 가 크다고 강한 선형관계를 나타내지는 않는다. 즉 해석과 비교가 어렵다.\n",
        "\n",
        "> $Cov(X, Y) = E[(X-EX)(Y-EY)] = E[XY -(EX)Y - X(EY) + (EX)(EY)]$\n",
        ">\n",
        "> $ = E(XY) - (EX)(EY) - (EX)(EY) + (EX)(EY)$\n",
        ">\n",
        "> $= E(XY) - (EX)(EY)$\n",
        "\n",
        "## 2. Covariance의 연산 성질\n",
        "1. $Cov(X,X) = Var(X)$\n",
        "2. $Cov(X,Y) = Cov(Y,X)$\n",
        "3. $Cov(X,c) = 0,\\space\\space c$ 는 상수\n",
        "4. $Cov(cX,Y) = cCov(X,Y)$\n",
        "5. $Cov(X,Y+Z)=Cov(X,Y) + Cov(X,Z)$\n",
        "6. $Cov(X+Y,Z+W) = Cov(X,Z) + Cov(X,W) + Cov(Y,Z) + Cov(Y,W)$\n",
        "  - $Cov(\\sum_{i}^{m}{a_i X_i}, \\sum_{j}^{n}{b_j Y_j}) = \\sum_{i,j}{a_i b_j Cov(X_i, Y_J)} $\n",
        "7. $Var(X_1+X_2) = Var(X_1) + Var(X_2) + 2Cov(X_1, X_2)$\n",
        "  - $Var(\\sum_{i}^{m}{X_i}) = Var(X_1) + Var(X_2)+...+Var(X_m) + 2\\sum_{i < j}{Cov(X_i, X_j)}$\n",
        "\n",
        "## 3. 독립과 Covariance의 관계\n",
        "- 확률변수 $X,Y$가 독립이면, $Cov(X,Y)=0$이다. \n",
        "- 하지만 그 역은 성립하지 않는다 : $Cov(X,Y)=0$ 이어도 $X,Y$는 독립이 아닐 수 있다.\n",
        "\n",
        "예시\n",
        "> $Z \\sim N(0,1)$이고, $X=Z, \\space\\space Y=Z^2$일 때를 생각해보자.\n",
        ">\n",
        "> $Cov(X, Y) = E(XY) - E(X)E(Y) = E(Z^3) - E(Z)E(Z^2)=0,\\space\\space$ $\\because Z$의 홀수차 적률은 0 \n",
        "\n",
        "\n",
        "# Correlation\n",
        "## 1. Definition\n",
        "- 해석과 비교가 어려운 Covariance의 단점을 보완하여 -1~1의 값을 가지도록 하는 척도. \n",
        "- 1에 가까울수록 강한 양의 상관관계를, -1에 가까울수록 강한 음의 상관관계를, 그리고 0에 가까울 수록 약한 상관관계를 나타낸다.\n",
        "> $Corr(X,Y) = \\frac{Cov(X, Y)}{SD(X)SD(Y)} = Cov( \\frac{X - EX}{SD(X)}, \\frac{Y - EY}{SD(Y)} )$\n",
        ">\n",
        "> 확률변수 $X, Y$를 standardization 한 후 Covariance를 계산한 것\n",
        "\n",
        "\n"
      ]
    }
  ]
}