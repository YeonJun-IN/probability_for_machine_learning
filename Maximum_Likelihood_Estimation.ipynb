{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maximum Likelihood Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMD1Mt4n8wZST0r8/5tHx3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeonJun-IN/probability_for_machine_learning/blob/main/Maximum_Likelihood_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqKT7sYZseRI"
      },
      "source": [
        "# MLE(Maximum Likelihodd Estimation)\n",
        "\n",
        "## Exponential Distribution\n",
        "\n",
        "#### Parameter Estimation \n",
        "\n",
        "> $PDF \\space\\space of \\space\\space Expo(\\lambda): \\space\\space f(x)=\\lambda e^{-\\lambda x}$\n",
        ">\n",
        "> $Joint \\space\\space PDF \\space\\space of \\space\\space Expo(\\lambda): \\space\\space f(x_1, x_2, ..., x_n|\\lambda)= \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i}$\n",
        ">\n",
        "> $L(\\lambda) = \\log {\\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i}} = \\sum_{i=1}^{n}{\\log{\\lambda e^{-\\lambda x_i}}} = n\\log{\\lambda} - \\sum_{i=1}^{n}{\\lambda x_i}$\n",
        ">\n",
        "> $\\frac{dL}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^{n}{x_i}=0$\n",
        ">\n",
        "> $\\frac{1}{\\lambda} = \\frac{1}{n}\\sum_{i=1}^{n}{x_i} = \\bar{X}$\n",
        "\n",
        "#### Check unbiasedness\n",
        "> $E(\\frac{1}{\\hat{\\lambda}}) =E(\\frac{1}{n}\\sum_{i=1}^{n}{x_i}) = \\frac{1}{n}E(x_1+x_2+...+x_n)$\n",
        "> $= E(x_1)$ by Linearity\n",
        "> \n",
        "> $X \\sim Expo(\\lambda)$ 일 때, $E(X) = \\frac{1}{\\lambda}$\n",
        ">\n",
        "> $E(\\frac{1}{\\hat{\\lambda}}) = \\frac{1}{\\lambda} \\space\\space \\therefore unbiased \\space\\space estimator$\n",
        "\n",
        "## Binomial Distribution\n",
        "#### Parameter Estimation\n",
        "> $PMF \\space \\space of \\space \\space Bin(n, p): \\space \\space  P(X) = {n\\choose k}p^k (1-p)^{n-k}$\n",
        ">\n",
        "> $Joint \\space \\space PMF \\space \\space of \\space \\space Bin(n, p): \\space \\space P(x_1, x_2,..., x_m | p) = \\prod_{i=1}^{m}{n \\choose x_i}p^{x_i} (1-p)^{n- x_i}$\n",
        ">\n",
        "> $L(p) = \\log {\\prod_{i=1}^{m}{n \\choose x_i}p^{x_i} (1-p)^{n- x_i}} = \\sum_{i=1}^{m}\\log{{n \\choose x_i}p^{x_i} (1-p)^{n- x_i}}$\n",
        ">\n",
        "> $= \\sum_{i=1}^{m}\\log{n \\choose x_i}  +\\log p \\sum_{i=1}^{m}x_i + \\log (1-p)\\sum_{i=1}^{m}(n-x_i)$\n",
        ">\n",
        "> $\\frac{dL}{dp} = \\frac{1}{p}\\sum_{i=1}^{m}x_i - \\frac{1}{1-p}\\sum_{i=1}^{m}(n-x_i)=\\frac{1}{p}\\sum_{i=1}^{m}x_i - \\frac{nm}{1-p}+\\frac{1}{1-p}\\sum_{i=1}^{m}x_i=0$\n",
        ">\n",
        "> $\\frac{1}{p(1-p)}\\sum_{i=1}^m x_i = \\frac{nm}{1-p}$\n",
        ">\n",
        "> $\\hat{p} = \\frac{1}{nm}\\sum_{i=1}^{m}x_i$\n",
        "#### Check unbiasedness\n",
        "> $E(\\hat{p})=E(\\frac{1}{nm}\\sum_{i=1}^{m}x_i) = \\frac{1}{nm}E(x_1 + x_2 + ... + x_m) =  \\frac{1}{n}E(x_1)$ by Linearity\n",
        ">\n",
        "> $X \\sim Bin(n, p)$일 때, $E(X)=np$\n",
        ">\n",
        "> $E(\\hat{p}) = \\frac{1}{n}np = p \\space\\space \\therefore unbiased \\space\\space estimator$\n",
        "## Poisson Distribution\n",
        "#### Parameter Estimation\n",
        "> $PMF \\space \\space of \\space \\space Pois(\\lambda): \\space \\space  P(X) = e^{-\\lambda} \\lambda ^k / k!$\n",
        ">\n",
        "> $Joint \\space \\space PMF \\space \\space of \\space \\space Pois(\\lambda): \\space \\space P(x_1, x_2,..., x_n | \\lambda) = \\prod_{i=1}^{n}e^{-\\lambda} \\lambda^{x_i} / x_i!$\n",
        ">\n",
        "> $L(\\lambda) = \\log {\\prod_{i=1}^{n}e^{-\\lambda} \\lambda^{x_i} / x_i!} = \\sum_{i=1}^{n}{\\log{e^{-\\lambda} \\lambda^{x_i} / x_i!}}= \\sum_{i=1}^{n}[{\\log{e^{-\\lambda}}} + \\log{\\lambda^{x_i}} - \\log{x_i!}]$\n",
        ">\n",
        "> $= -\\lambda n +\\log{\\lambda}\\sum_{i=1}^n {x_i} - \\sum_{i=1}^n{\\log{x_i!}}$\n",
        ">\n",
        "> $\\frac{dL}{d\\lambda} = -n + \\frac{1}{\\lambda}\\sum_{i=1}^n {x_i}=0$\n",
        ">\n",
        "> $\\lambda = \\frac{1}{n}\\sum_{i=1}^n {x_i}$\n",
        "#### Check unbiasedness\n",
        "> $E(\\hat{\\lambda})=E(\\frac{1}{n}\\sum_{i=1}^n {x_i}) = \\frac{1}{n}E(x_1 + x_2 + ... + x_n) = E(x_1)$ by Linearity\n",
        ">\n",
        "> $X \\sim Pois(\\lambda)$일 때, $E(X)=\\lambda$\n",
        ">\n",
        "> $E(\\hat{\\lambda}) = \\lambda \\space\\space \\therefore unbiased \\space\\space estimator$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L-DvGpNwDRc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}